#!/usr/bin/env python3
import html
import signal
import datetime
import sys
import toml
from beautifultable import BeautifulTable

import logger
from commandparser import CommandParser
from incident import Incident
from pageduty import Pagerduty
from confluence import Confluence

# Suppress annoying warning when importing from fuzzywuzzy package
import warnings

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    from fuzzywuzzy import process

# Somthing for control-c
signal.signal(signal.SIGINT, lambda s, f: (logger.info("Bye now"), sys.exit(130)))

date_format = '%Y-%m-%d'


def main():
    options = CommandParser()
    logger.debug_flag = options.debug

    config = load_config()

    # Read Pagerduty API key from flags or config file
    key = options.key if options.key else config.get('pagerduty', {}).get('api_token')

    try:
        if key is None:
            logger.fatal("No key found in command options or config file")
        else:
            logger.info(f"Using Pagerduty Service IDs {','.join(options.services)}"
                        f" with Team IDs {','.join(options.teams)}")

        logger.info(f"Searching for incidents >= {options.start.strftime(date_format)} 00:00:00 "
                    f"< {options.end.strftime(date_format)} 00:00:00")

        incidents = fetch_incidents(Pagerduty(key, options.teams, options.services), options)

        if len(incidents) == 0:
            logger.info("No incidents found")
            sys.exit(0)

        if options.report:
            full_report(incidents, options, config)
        else:
            single_report(incidents, options, config)

    except Exception as err:
        logger.fatal(err)


def full_report(incidents, options, config):
    if options.format == CommandParser.FORMAT_CONFLUENCE:
        logger.fatal("Do something special for Confluence full report")

    output(sort_results(transform_flat(incidents, options), 'service', 'description'), options, config)
    options.group = "Description"
    output(transform_grouped(incidents, options), options, config)
    options.group = "Ack"
    output(transform_grouped(incidents, options), options, config)


def single_report(incidents, options, config):
    if options.group is None:
        data = sort_results(transform_flat(incidents, options), 'service', 'description')
        output(data, options, config)
    else:
        output(transform_grouped(incidents, options), options, config)


def output(data, options, config):
    if options.format == CommandParser.FORMAT_CSV:
        output_csv(data)
    elif options.format == CommandParser.FORMAT_CONFLUENCE:
        output_confluence(data, options, config)
    else:
        output_pretty(data)


def load_config():
    from os.path import expanduser
    config_file = f"{expanduser('~')}/.pd.toml"
    config = None

    f = None
    try:
        f = open(config_file)
        config = toml.load(f)
    except FileNotFoundError:
        pass  # Ignore no file found
    except Exception as err:
        logger.fatal(f"Error reading config file {config_file} - {err}")
    finally:
        f.close()

    return config


def escape_quotes(s):
    return s.replace('"', '\\"')


def output_confluence(data, options, config):
    title = f"On Call {options.start.strftime(date_format)}-{options.end.strftime(date_format)}"
    body = format_confluence_table_markdown(data)

    confluence = Confluence(config['confluence']['user_email'], config['confluence']['api_token'])
    resp = confluence.create_page(config['confluence']['space_key'], title, body)
    logger.info(f"Find your Confluence page here: ${resp['_links']['base']}{resp['_links']['webui']}")


def format_confluence_table_markdown(data):
    markdown = ""

    for i, row in enumerate(data):
        line = ""

        divider = '||' if i == 0 else '|'

        for c in row:
            if type(c) == str:
                c = c.replace('|', '\\|').replace('[', '\\[').replace('{', '\\{')

            line += f"{divider}{c}"

        line += divider if line != "" else ''
        markdown += f"{line}\n"

    return markdown


def output_pretty(flat_data):
    table = BeautifulTable(maxwidth=180)
    table.set_style(BeautifulTable.STYLE_BOX_ROUNDED)
    table.columns.header = flat_data[0]

    for row in flat_data[1:]:
        table.rows.append(row)

    print(table)


def output_csv(data):
    for row in data:
        line = ""
        for item in row:
            line += f"\"{escape_quotes(str(item))}\","
        print(line.rstrip(","))


def transform_flat(incidents, options):
    flat = [["Service", "Team", "Incident", "Created", "Description", "Ack", "Urgency", "Resolution Time"]]

    for i in incidents:

        duration_string = '-'
        if i.resolution_time():
            duration_string = str(datetime.timedelta(seconds=i.resolution_time())).split('.')[0]  # remove microseconds

        flat.append([
            i.service(),
            i.team(),
            i.incident_number(),
            i.created_at(),
            i.description().replace("\n", " ") if options.full else i.short_description().replace("\n", " "),
            i.acknowledged_by(),
            i.urgency(),
            duration_string
        ])

    return flat


def column_index(column_names, column_to_find):
    # Find index of the grouping field. Case-insensitive match on user specified field name
    try:
        index = [col.lower() for col in column_names].index(column_to_find.lower())
    except ValueError:
        raise RuntimeError(f"Unable to find column: {column_to_find}")

    return index


def transform_grouped(incidents, options):
    flat = transform_flat(incidents, options)

    grouper_index = column_index(flat[0], options.group)

    groups = {}
    fuzzy_groups = {}

    for i in flat[1:]:
        key = i[grouper_index]

        if key not in groups:

            if options.fuzz:
                # Get current groups
                current_groups = groups.keys()

                # Get the best match
                if len(current_groups) > 0:
                    best_match, score = process.extractOne(key, current_groups)

                    # If the match is good keep track of the columns where we have fuzzed
                    # and group on existing key
                    if score >= options.fuzz_match:
                        logger.debug(f"{score} > {options.fuzz_match}. Matching {key} and {best_match}")

                        if best_match in fuzzy_groups:
                            fuzzy_groups[best_match] += 1
                        else:
                            fuzzy_groups[best_match] = 1

                        key = best_match

        # Add either exact match or fuzzy key to dict
        groups[key] = groups[key] + 1 if key in groups else 1

    # Where a group was achieved through fuzzing add a prefix to the key, so it's clearer what happened in output
    for group in fuzzy_groups.keys():
        groups[f"{group} [f{fuzzy_groups[group]}]"] = groups.pop(group)

    # Add headers to grouped output
    grouped_flat = [[options.group.title(), "Count"]]

    for group, count in sorted(groups.items(), key=lambda item: item[1], reverse=True):
        grouped_flat.append([
            group,
            count
        ])

    return grouped_flat


def sort_results(data, col1, col2):
    col1_index = column_index(data[0], col1)
    col2_index = column_index(data[0], col2)
    sorted_data = sorted(data[1:], key=lambda x: (x[col1_index], x[col2_index]))
    return [data[0]] + sorted_data


def fetch_incidents(pg, options) -> []:
    """Fetch PagerDuty incidents, log_entries, notes"""

    # Incidents with log entries and notes
    full_incidents = []

    incidents = pg.fetch_raw_incidents(options.start, options.end)

    logger.info(f"Incidents found: {len(incidents)}")

    for i in incidents:

        # Reading incident logs os slow only do it if requested
        if options.logs:
            log_entries = pg.fetch_incident_log(i['id'])
            reading_log = f"{len(log_entries)} log entries read"
        else:
            reading_log = ""
            log_entries = []

        if options.notes:
            notes = pg.fetch_incident_notes(i['id'])
            notes_log = f"{len(notes)} notes found"
        else:
            notes_log = ""
            notes = []

        if options.logs or options.notes:
            divider = ' | ' if reading_log and notes_log else ''
            logger.info(f"Incident {i['incident_number']} {reading_log}{divider}{notes_log}")

        full_incidents.append(Incident(i, log_entries, notes))

    return full_incidents


if __name__ == "__main__":
    main()
